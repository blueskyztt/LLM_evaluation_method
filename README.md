The evaluation indicators of our survey are as follows:
- [BLEU](BLEU.md): BLEU is used to evaluate machine translation tasks
- [CHRF](./CHRF.md): CHRF is used to evaluate text generation tasks
- [macro F1](./pdf/macro_F1.pdf): Macro F1 is used to evaluate multi-classification problems
- [MCC](./pdf/MCC.pdf): MCC is used to evaluate binary classification models
- [METROR](./pdf/METROR.pdf): METROR is used to evaluate machine translation tasks
- [MRR](./pdf/MRR.pdf): MRR is used to evaluate information retrieval tasks and ranking tasks
- [Perplexity](./pdf/Perplexity.pdf): Perplexity is used to evaluate language models
- [Precision](./pdf/Precision,Recall,F1.pdf): Precision is used to evaluate classification models, information retrieval tasks, etc.
- [Recall](./pdf/Precision,Recall,F1.pdf): Recall is used to evaluate classification models, information retrieval tasks, etc.
- [F1](./pdf/Precision,Recall,F1.pdf): $F_1$ is used to evaluate classification models, information retrieval tasks, etc.
- [ROUGE](./pdf/ROUGE.pdf): ROUGE is used to evaluate text generation tasks
